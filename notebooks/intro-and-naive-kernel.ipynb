{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642c5f84-4853-441e-869e-4966bee0d84b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This workshop will go over some ways to think about optimizing kernels. We'll do\n",
    "this through the lens of matrix multiplication. \n",
    "\n",
    "Matrix multiplication is a good candidate to learn the fundamentals of kernel\n",
    "optimization for a number of reasons:\n",
    "\n",
    "1. Matrix multiplication is \"compute bound\", meaning the speed at which a chip\n",
    "   can perform floating point operations is the limiting factor\n",
    "2. Matrix multiplication is straightforward to implement and think about\n",
    "3. Matrix multiplication is widely used, and other linear algebra operations can\n",
    "   be recast as matrix multiplication\n",
    "   (see: [Matricization](https://en.wikipedia.org/wiki/Tensor_reshaping#Mode-m_Flattening_/_Mode-m_Matrixization))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792ae71-811c-47b9-83e2-345732b286f8",
   "metadata": {},
   "source": [
    "# Quick GPU architecture overview\n",
    "\n",
    "NVIDIA Hopper chip (GH100):\n",
    "\n",
    "<img src=\"../resources/hopper-full-gpu.png\" width=800 />\n",
    "\n",
    "- Image taken from [NVIDIA Hopper architecture blogpost](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/)\n",
    "\n",
    "\n",
    "## Simplified view:\n",
    "<img src=\"../resources/CPUGPU.png\" width=500 />\n",
    "\n",
    "- Image taken from the [CUDA programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
    "\n",
    "- GPUs trade fancy features for massive amounts of compute units\n",
    "- CPUs trade compute for fancy features (think branch prediction)\n",
    "\n",
    "## A closer look at GPUs: Streaming Multiprocessors (SMs)\n",
    "\n",
    "NVIDIA Hopper SM architecture (GH100):\n",
    "\n",
    "<img src=\"../resources/hopper-sm-arch.png\" width=500 />\n",
    "\n",
    "- Image taken from [NVIDIA Hopper architecture blogpost](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/)\n",
    "\n",
    "- SMs are the \"core\" of the GPU\n",
    "    - Commonly confused with the compute units (\"CUDA cores\")\n",
    "- Each GPU has a number of SMs\n",
    "    - H100 SM count: 114\n",
    "    - A100 SM count: 108\n",
    "- Getting \"performance\" out of a GPU usually means writing code in such a way\n",
    "  that SMs are as busy as possible and running as efficiently as possible with\n",
    "  respect to memory accesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02c6e5-23c8-4346-a9fc-0ed8fc4ffeb8",
   "metadata": {},
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "Given $A \\in \\mathbb{R}^{n \\times n}, B \\in \\mathbb{R}^{n \\times n}$ and\n",
    "$C \\in \\mathbb{R}^{n \\times n}$\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3b720-a36e-4d54-af02-7f2928c6d223",
   "metadata": {},
   "source": [
    "# Runner utility\n",
    "To make it so we don't need to write a driver in C++, we have some helper classes\n",
    "that will use [`cupy`]() to run the kernels from our notebooks. There are blank\n",
    "kernels where we will be filling in details like linearized index expressions and\n",
    "loading into shared memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfefd0-09e3-400c-9c87-775e95bce1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cu\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class KernelRunner:\n",
    "    kernel_name: str\n",
    "    template: str\n",
    "\n",
    "    def _get_kernel_src(self, read_full_src: bool = False) -> str:\n",
    "        cur_dir = os.getcwd()\n",
    "\n",
    "        kernel_dir = \"full\" if read_full_src else \"blank\"\n",
    "\n",
    "        with open(\n",
    "            f\"{cur_dir}/../src/kernels/{kernel_dir}/{self.kernel_name}.cu\",\n",
    "            \"r\") as f:\n",
    "            src = f.read()\n",
    "\n",
    "        return src\n",
    "\n",
    "    def __call__(self,\n",
    "                 block_dim: tuple[int, ...],\n",
    "                 grid_dim: tuple[int, ...],\n",
    "                 args: tuple,\n",
    "                 read_full_src: bool = False,\n",
    "                 niterations: int = 1) -> np.ndarray:\n",
    "\n",
    "        N, = args\n",
    "\n",
    "        src = self._get_kernel_src(read_full_src)\n",
    "\n",
    "        wrapper_name = f\"{self.kernel_name}_float\"\n",
    "        code = src + f\"\"\"\n",
    "extern \"C\" __global__\n",
    "void {wrapper_name}(const float *__restrict__ A,\n",
    "                    const float *__restrict__ B,\n",
    "                    float *__restrict__ C,\n",
    "                    const int N) {{\n",
    "  {self.kernel_name}_matmul{self.template}(A, B, C, N);\n",
    "}}\n",
    "        \"\"\"\n",
    "\n",
    "        mod = cu.RawModule(code=code,\n",
    "                           options=(\"-std=c++14\",),\n",
    "                           name_expressions=[wrapper_name])\n",
    "\n",
    "        func = mod.get_function(wrapper_name)\n",
    "\n",
    "        A = cu.random.rand(N, N, dtype=cu.float32)\n",
    "        B = cu.random.rand(N, N, dtype=cu.float32)\n",
    "        C = cu.zeros_like(A)\n",
    "\n",
    "        C_true = A @ B\n",
    "\n",
    "        if niterations == 1:\n",
    "            func(grid_dim, block_dim, (A, B, C, N))\n",
    "            cu.cuda.runtime.deviceSynchronize()\n",
    "            print(f\"Error = {cu.linalg.norm(C_true - C) / cu.linalg.norm(C):.4f}\")\n",
    "        else:\n",
    "            # warm up calls\n",
    "            for _ in range(5):\n",
    "                func(grid_dim, block_dim, (A, B, C, N))\n",
    "            cu.cuda.runtime.deviceSynchronize()\n",
    "\n",
    "            start = cu.cuda.Event()\n",
    "            end = cu.cuda.Event()\n",
    "\n",
    "            start.record()\n",
    "            for _ in range(niterations):\n",
    "                func(grid_dim, block_dim, (A, B, C, N))\n",
    "            end.record()\n",
    "            end.synchronize()\n",
    "\n",
    "            ms = cu.cuda.get_elapsed_time(start, end)\n",
    "\n",
    "            print(10*\"=\", self.kernel_name, 10*\"=\")\n",
    "            print(f\"Total time  : {ms:.4f} ms\")\n",
    "            print(f\"Average time: {ms / niterations:.4f} ms\")\n",
    "\n",
    "            # FIXME: compute observed GFLOP/s\n",
    "            \n",
    "            # HINT: you'll need total flops, and to convert ms to s\n",
    "            # per iteration\n",
    "            \n",
    "            # NOTE: this is just a rough estimate. if you want a better pulse on\n",
    "            # how a kernel is performing, consider using NVIDIA Nsight Compute to\n",
    "            # profile\n",
    "            gflops = 0.0\n",
    "            print(f\"GFLOP/s     : {gflops:.4f}\")\n",
    "            print(f\"Error       : {cu.linalg.norm(C_true - C) / cu.linalg.norm(C):.4f}\")\n",
    "            print((22 + len(self.kernel_name))*\"=\")\n",
    "\n",
    "        return C.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5cedcd-e9ee-48cb-a55f-354ca0889f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom runner:\n",
    "@dataclass(frozen=True)\n",
    "class NaiveKernelRunner(KernelRunner):\n",
    "    template: str = \"<float>\"\n",
    "    kernel_name: str = \"naive\"\n",
    "\n",
    "# usage would be something like this:\n",
    "N = 1024\n",
    "BM = BN = 32\n",
    "block_dim = (BM, BN)\n",
    "grid_dim = (N // BM, N // BN)\n",
    "\n",
    "runner = NaiveKernelRunner()\n",
    "\n",
    "# NOTE: for kernels we're running, set read_full_src=False to use the kernel \n",
    "# you edit/write \n",
    "C = runner(block_dim, grid_dim, (N,), read_full_src=True, niterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a55ba-e3e9-4518-9fb4-4083559fd454",
   "metadata": {},
   "source": [
    "# Data storage formats\n",
    "\n",
    "Arrays are multidimensional, but memory is linear. There is no \"right answer\" to\n",
    "how to linearize arrays in memory.\n",
    "\n",
    "The two dominant ways of storing to memory are \"row-major\" (C-order, C for C) and \n",
    "\"column-major\" (F-order, F for Fortran).\n",
    "\n",
    "For row major, we take the rows of our array and lay them out from beginning to\n",
    "end, one after another.\n",
    "\n",
    "For column major, we instead move down the columns one after another.\n",
    "\n",
    "<img src=\"../../resources/array-layout.png\" width=300 />\n",
    "\n",
    "Our arrays are *row-major*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2ff08a-f33e-4da1-8647-3d6dd734f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i, j) order took: 0.7688 s\n",
      "(j, i) order took: 1.0204 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "N = 2048 \n",
    "a = np.random.randn(N, N)\n",
    "acc = 0.0\n",
    "start = time.time()\n",
    "# write a loop that accumulates a[i,j] using (i, j) loop order\n",
    "end = time.time()\n",
    "print(f\"(i, j) order took: {end - start:.4f} s\")\n",
    "\n",
    "acc = 0.0\n",
    "start = time.time()\n",
    "# write a loop that accumulates a[i,j] using (j, i) loop order\n",
    "end = time.time()\n",
    "print(f\"(j, i) order took: {end - start:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd212738-ed43-4e24-96a6-3a5bda7edb0f",
   "metadata": {},
   "source": [
    "# Naive implementation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec28689-d8cc-4290-9593-7055ffe5be92",
   "metadata": {},
   "source": [
    "Go to <a href=\"../src/kernels/blank/naive.cu\">the blank `naive.cu` kernel</a>. \n",
    "We'll start by writing the necessary pieces to get up and running. We'll need\n",
    "to fill in all the `FIXME`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95965705-b635-4460-b4a3-97359f470432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the naive runner:\n",
    "@dataclass(frozen=True)\n",
    "class NaiveKernelRunner(KernelRunner):\n",
    "    template: str = \"<float>\"\n",
    "    kernel_name: str = \"naive\"\n",
    "\n",
    "# the kernels do not have bounds checking to reduce visual noise, so we need to\n",
    "# choose N, BM, and BN such that N is evenly divisible by BM and BN\n",
    "# some values to try: 1024, 2048, 4096, 8192\n",
    "N = ...\n",
    "BN = BM = ...\n",
    "block_dim = ...\n",
    "grid_dim = ...\n",
    "runner = ...\n",
    "C = runner(block_dim, grid_dim, (N,), read_full_src=False, niterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118812ae-5385-487e-a9c0-9248b414ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "clock_speed_mhz = 1695.0\n",
    "clocks_per_sec = ...\n",
    "flops_per_clock = ...\n",
    "flops_per_sec = ...\n",
    "\n",
    "gb_per_sec = 936.2\n",
    "bytes_per_sec = ...\n",
    "\n",
    "arithmetic_intensity = flops_per_sec / bytes_per_sec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
